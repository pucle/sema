{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# üö¶ Semaphore Detector - Google Colab Server\n",
                "\n",
                "**Run AI inference with FREE GPU on Google Colab!**\n",
                "\n",
                "This notebook sets up:\n",
                "1. FastAPI server for webcam frame processing\n",
                "2. ngrok tunnel for public access\n",
                "3. GPU-accelerated inference\n",
                "\n",
                "---\n",
                "\n",
                "## üìã Instructions:\n",
                "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
                "2. **Run all cells** in order\n",
                "3. **Copy the ngrok URL** and paste it in your frontend config.js\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "step1"
            },
            "source": [
                "## Step 1: Check GPU Status"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "check_gpu"
            },
            "outputs": [],
            "source": [
                "# Check if GPU is available\n",
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"\\n‚úÖ PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"üìπ GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "step2"
            },
            "source": [
                "## Step 2: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "%%capture\n",
                "# Install required packages\n",
                "!pip install fastapi uvicorn python-multipart pyngrok nest_asyncio\n",
                "!pip install opencv-python-headless numpy pillow\n",
                "!pip install inference\n",
                "\n",
                "print(\"‚úÖ All dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "step3"
            },
            "source": [
                "## Step 3: Configure ngrok\n",
                "\n",
                "‚ö†Ô∏è **Get your free ngrok authtoken:**\n",
                "1. Go to [ngrok.com](https://ngrok.com) and sign up (free)\n",
                "2. Copy your authtoken from [dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken)\n",
                "3. Paste it below"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "ngrok_config"
            },
            "outputs": [],
            "source": [
                "# PASTE YOUR NGROK AUTHTOKEN HERE\n",
                "NGROK_AUTHTOKEN = \"\"  # <-- Paste your token here\n",
                "\n",
                "if not NGROK_AUTHTOKEN:\n",
                "    print(\"‚ö†Ô∏è Please set your ngrok authtoken above!\")\n",
                "    print(\"üìç Get it free at: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
                "else:\n",
                "    from pyngrok import ngrok\n",
                "    ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
                "    print(\"‚úÖ ngrok configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "step4"
            },
            "source": [
                "## Step 4: Create the FastAPI Server"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "create_server"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import time\n",
                "import base64\n",
                "from datetime import datetime\n",
                "from typing import Optional, Dict, List, Any\n",
                "from collections import defaultdict\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "\n",
                "from fastapi import FastAPI, WebSocket, WebSocketDisconnect, File, UploadFile, Form, Query\n",
                "from fastapi.middleware.cors import CORSMiddleware\n",
                "from fastapi.responses import JSONResponse\n",
                "\n",
                "# ============================================================\n",
                "# CONFIGURATION - Edit these if needed\n",
                "# ============================================================\n",
                "\n",
                "API_KEY = \"ylFu6Gi5msSoDxbPC9Sl\"  # Your Roboflow API key\n",
                "MODEL_ID = \"semaphore-dataset-1wlaa/1\"  # Your model ID\n",
                "CONFIDENCE_THRESHOLD = 0.60\n",
                "\n",
                "# ============================================================\n",
                "# LOAD MODEL\n",
                "# ============================================================\n",
                "\n",
                "print(\"üîÑ Loading model...\")\n",
                "\n",
                "# Enable GPU for inference\n",
                "os.environ[\"ROBOFLOW_INFERENCE_DEVICE\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"üìç Device: {os.environ['ROBOFLOW_INFERENCE_DEVICE']}\")\n",
                "\n",
                "from inference import get_model\n",
                "model = get_model(model_id=MODEL_ID, api_key=API_KEY)\n",
                "print(f\"‚úÖ Model loaded: {MODEL_ID}\")\n",
                "\n",
                "# ============================================================\n",
                "# CREATE FASTAPI APP\n",
                "# ============================================================\n",
                "\n",
                "app = FastAPI(title=\"Semaphore Detector API\")\n",
                "\n",
                "app.add_middleware(\n",
                "    CORSMiddleware,\n",
                "    allow_origins=[\"*\"],\n",
                "    allow_credentials=True,\n",
                "    allow_methods=[\"*\"],\n",
                "    allow_headers=[\"*\"],\n",
                ")\n",
                "\n",
                "# Session storage\n",
                "session_results: Dict[str, Dict[str, Any]] = defaultdict(dict)\n",
                "\n",
                "@app.get(\"/\")\n",
                "async def root():\n",
                "    return {\n",
                "        \"status\": \"running\",\n",
                "        \"service\": \"Semaphore Detector API (Colab)\",\n",
                "        \"device\": os.environ.get(\"ROBOFLOW_INFERENCE_DEVICE\", \"unknown\"),\n",
                "        \"timestamp\": datetime.now().isoformat()\n",
                "    }\n",
                "\n",
                "@app.get(\"/health\")\n",
                "async def health():\n",
                "    return {\"status\": \"healthy\", \"model_loaded\": True}\n",
                "\n",
                "@app.post(\"/api/process-frame\")\n",
                "async def process_frame(\n",
                "    file: UploadFile = File(...),\n",
                "    session_id: Optional[str] = Form(None)\n",
                "):\n",
                "    start_time = time.time()\n",
                "    \n",
                "    try:\n",
                "        # Read image\n",
                "        contents = await file.read()\n",
                "        nparr = np.frombuffer(contents, np.uint8)\n",
                "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
                "        \n",
                "        if img is None:\n",
                "            return {\"error\": \"Invalid image\", \"detections\": []}\n",
                "        \n",
                "        # Run inference\n",
                "        results = model.infer(img)[0]\n",
                "        predictions = results.predictions\n",
                "        \n",
                "        # Format detections\n",
                "        detections = []\n",
                "        for pred in predictions:\n",
                "            if hasattr(pred, 'class_name'):\n",
                "                confidence = float(pred.confidence)\n",
                "                if confidence >= CONFIDENCE_THRESHOLD:\n",
                "                    detections.append({\n",
                "                        \"class\": pred.class_name,\n",
                "                        \"confidence\": confidence,\n",
                "                        \"x\": float(pred.x),\n",
                "                        \"y\": float(pred.y),\n",
                "                        \"width\": float(pred.width),\n",
                "                        \"height\": float(pred.height),\n",
                "                        \"bbox\": [\n",
                "                            float(pred.x - pred.width / 2),\n",
                "                            float(pred.y - pred.height / 2),\n",
                "                            float(pred.x + pred.width / 2),\n",
                "                            float(pred.y + pred.height / 2)\n",
                "                        ]\n",
                "                    })\n",
                "        \n",
                "        result_data = {\n",
                "            \"detections\": detections,\n",
                "            \"timestamp\": time.time(),\n",
                "            \"latency\": (time.time() - start_time) * 1000\n",
                "        }\n",
                "        \n",
                "        if session_id:\n",
                "            session_results[session_id] = result_data\n",
                "        \n",
                "        return result_data\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error: {e}\")\n",
                "        return {\"error\": str(e), \"detections\": []}\n",
                "\n",
                "@app.get(\"/api/latest-results\")\n",
                "async def get_latest_results(session: str = Query(...)):\n",
                "    if session in session_results:\n",
                "        return session_results[session]\n",
                "    return {\"detections\": [], \"message\": \"No results yet\"}\n",
                "\n",
                "@app.websocket(\"/ws/stream\")\n",
                "async def websocket_endpoint(websocket: WebSocket):\n",
                "    await websocket.accept()\n",
                "    \n",
                "    try:\n",
                "        while True:\n",
                "            data = await websocket.receive_text()\n",
                "            start_time = time.time()\n",
                "            \n",
                "            try:\n",
                "                # Parse base64 image\n",
                "                if ',' in data:\n",
                "                    header, encoded = data.split(',', 1)\n",
                "                else:\n",
                "                    encoded = data\n",
                "                \n",
                "                img_data = base64.b64decode(encoded)\n",
                "                nparr = np.frombuffer(img_data, np.uint8)\n",
                "                img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
                "                \n",
                "                if img is None:\n",
                "                    await websocket.send_json({\"error\": \"Invalid image\", \"detections\": []})\n",
                "                    continue\n",
                "                \n",
                "                # Run inference\n",
                "                results = model.infer(img)[0]\n",
                "                predictions = results.predictions\n",
                "                \n",
                "                detections = []\n",
                "                for pred in predictions:\n",
                "                    if hasattr(pred, 'class_name'):\n",
                "                        confidence = float(pred.confidence)\n",
                "                        if confidence >= CONFIDENCE_THRESHOLD:\n",
                "                            detections.append({\n",
                "                                \"class\": pred.class_name,\n",
                "                                \"confidence\": confidence,\n",
                "                                \"x\": float(pred.x),\n",
                "                                \"y\": float(pred.y),\n",
                "                                \"width\": float(pred.width),\n",
                "                                \"height\": float(pred.height),\n",
                "                                \"bbox\": [\n",
                "                                    float(pred.x - pred.width / 2),\n",
                "                                    float(pred.y - pred.height / 2),\n",
                "                                    float(pred.x + pred.width / 2),\n",
                "                                    float(pred.y + pred.height / 2)\n",
                "                                ]\n",
                "                            })\n",
                "                \n",
                "                latency = (time.time() - start_time) * 1000\n",
                "                await websocket.send_json({\n",
                "                    \"detections\": detections,\n",
                "                    \"timestamp\": time.time(),\n",
                "                    \"latency\": latency\n",
                "                })\n",
                "                \n",
                "            except Exception as e:\n",
                "                await websocket.send_json({\"error\": str(e), \"detections\": []})\n",
                "                \n",
                "    except WebSocketDisconnect:\n",
                "        print(\"üì° Client disconnected\")\n",
                "\n",
                "print(\"‚úÖ FastAPI app created!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "step5"
            },
            "source": [
                "## Step 5: Start Server with ngrok Tunnel\n",
                "\n",
                "üöÄ **After running this cell, copy the ngrok URL and use it in your frontend!**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "start_server"
            },
            "outputs": [],
            "source": [
                "import nest_asyncio\n",
                "from pyngrok import ngrok\n",
                "import uvicorn\n",
                "\n",
                "# Apply nest_asyncio for Colab compatibility\n",
                "nest_asyncio.apply()\n",
                "\n",
                "# Start ngrok tunnel\n",
                "PORT = 8000\n",
                "public_url = ngrok.connect(PORT)\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"üöÄ SEMAPHORE DETECTOR SERVER IS RUNNING!\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\")\n",
                "print(f\"üìç PUBLIC URL: {public_url}\")\n",
                "print(f\"\")\n",
                "print(f\"üìã Copy this URL to your frontend config.js:\")\n",
                "print(f\"   BACKEND_URL: '{public_url}'\")\n",
                "print(f\"   WS_URL: '{str(public_url).replace('https://', 'wss://').replace('http://', 'ws://')}/ws/stream'\")\n",
                "print(f\"\")\n",
                "print(\"=\" * 60)\n",
                "print(\"‚è≥ Server is running... Keep this cell running!\")\n",
                "print(\"   Press STOP to shut down the server.\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Run server\n",
                "uvicorn.run(app, host=\"0.0.0.0\", port=PORT)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "troubleshooting"
            },
            "source": [
                "---\n",
                "\n",
                "## üîß Troubleshooting\n",
                "\n",
                "### Session Timeout\n",
                "- Free Colab sessions last up to 12 hours\n",
                "- GPU sessions may timeout after ~3 hours of inactivity\n",
                "- Keep the browser tab open and occasionally interact\n",
                "\n",
                "### ngrok Errors\n",
                "- Make sure you set your authtoken in Step 3\n",
                "- Free tier allows 1 active tunnel at a time\n",
                "- The URL changes each time you restart\n",
                "\n",
                "### Model Loading Errors\n",
                "- Check your Roboflow API key is correct\n",
                "- Verify the model ID exists in your Roboflow account\n",
                "\n",
                "### Connection Issues\n",
                "- The public URL may take a few seconds to become active\n",
                "- Try refreshing your frontend page\n",
                "- Check if CORS is properly configured"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}